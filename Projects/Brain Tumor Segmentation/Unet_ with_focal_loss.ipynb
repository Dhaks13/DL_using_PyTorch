{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9049603,"sourceType":"datasetVersion","datasetId":5456201}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dhaks13/brain-tumor-segmentation-unet-focalloss?scriptVersionId=190224511\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Importing Modules","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:59:38.418201Z","iopub.execute_input":"2024-07-28T16:59:38.41893Z","iopub.status.idle":"2024-07-28T16:59:38.425184Z","shell.execute_reply.started":"2024-07-28T16:59:38.418897Z","shell.execute_reply":"2024-07-28T16:59:38.424124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:44:59.916788Z","iopub.execute_input":"2024-07-28T16:44:59.917144Z","iopub.status.idle":"2024-07-28T16:44:59.923315Z","shell.execute_reply.started":"2024-07-28T16:44:59.917105Z","shell.execute_reply":"2024-07-28T16:44:59.922237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data and split it into train and test","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 4 # Set the batch size\nROOT_DIR = '/kaggle/input/brain-tumor-segmentation/data/Glioma' # Set the root directory to the Glioma dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:44:59.924911Z","iopub.execute_input":"2024-07-28T16:44:59.925517Z","iopub.status.idle":"2024-07-28T16:44:59.930556Z","shell.execute_reply.started":"2024-07-28T16:44:59.925484Z","shell.execute_reply":"2024-07-28T16:44:59.929621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Dataset class\nclass BrainTumorDataset(Dataset):\n    def __init__(self,images, root_dir, transform=None):\n        self.images = images \n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image_name = self.images[idx]\n        img_path = os.path.join(self.root_dir, image_name)\n        mask_name = image_name.replace('.png', '_mask.png')\n        mask_path = os.path.join(self.root_dir, mask_name)\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n            mask = self.transform(mask)\n\n        return image, mask\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:44:59.933492Z","iopub.execute_input":"2024-07-28T16:44:59.933829Z","iopub.status.idle":"2024-07-28T16:44:59.941782Z","shell.execute_reply.started":"2024-07-28T16:44:59.933795Z","shell.execute_reply":"2024-07-28T16:44:59.940983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the images and masks\n\n# Define the transformations\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Get the list of images\nall_images = [file for file in os.listdir(ROOT_DIR) if file.endswith('.png') and '_mask' not in file]\n\n# Split the images into training and testing sets\ntrain_files, test_files = train_test_split(all_images, test_size=0.2, random_state=42)\n\n# Create the training and testing datasets\ntrain_dataset = BrainTumorDataset(train_files, ROOT_DIR, transform=transform)\ntest_dataset = BrainTumorDataset(test_files, ROOT_DIR, transform=transform)\n\n# Create the training and testing dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:44:59.943004Z","iopub.execute_input":"2024-07-28T16:44:59.943414Z","iopub.status.idle":"2024-07-28T16:44:59.956375Z","shell.execute_reply.started":"2024-07-28T16:44:59.943384Z","shell.execute_reply":"2024-07-28T16:44:59.955453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display a few images and masks\ndef display_image_mask(images, masks):\n    fig,axes = plt.subplots(2,4,figsize=(12,6))\n    for i in range(4):\n        axes[0,i].imshow(images[i].permute(1, 2, 0))\n        axes[0, i].set_title('Image')\n        axes[0,i].axis('off')\n        axes[1,i].imshow(masks[i][0], cmap='gray')\n        axes[1, i].set_title('Mask')\n        axes[1,i].axis('off')\n    plt.show()\n\n# Get a batch of training data\nimages, masks = next(iter(train_loader))\nprint(f\"Training Batch - Image Shape: {images.shape}, Mask Shape: {masks.shape}\")\ndisplay_image_mask(images, masks)\n\n# Get a batch of testing data\nimages, masks = next(iter(test_loader))\nprint(f\"Testing Batch - Image Shape: {images.shape}, Mask Shape: {masks.shape}\")\ndisplay_image_mask(images, masks)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:44:59.957681Z","iopub.execute_input":"2024-07-28T16:44:59.95805Z","iopub.status.idle":"2024-07-28T16:45:01.715933Z","shell.execute_reply.started":"2024-07-28T16:44:59.958023Z","shell.execute_reply":"2024-07-28T16:45:01.715043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the number of batches in the train and test loaders\nprint(f\"Number of batches in train loader: {len(train_loader)}\")\nprint(f\"Number of batches in test loader: {len(test_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:45:01.717162Z","iopub.execute_input":"2024-07-28T16:45:01.717493Z","iopub.status.idle":"2024-07-28T16:45:01.723473Z","shell.execute_reply.started":"2024-07-28T16:45:01.717465Z","shell.execute_reply":"2024-07-28T16:45:01.721735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"markdown","source":"### Parts of U-Net","metadata":{}},{"cell_type":"code","source":"# Convolution Block\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels,padding = 0):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# Downsample Block \nclass DownsampleBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = ConvBlock(in_channels, out_channels)\n        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pool(x)\n        return x\n\n# Upsample Block\nclass UpsampleBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.upsample = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n        self.conv = ConvBlock(out_channels*2, out_channels)\n    \n    def forward(self, x, down_tensor):\n        x = self.upsample(x)\n        diffY = down_tensor.size()[2] - x.size()[2]\n        diffX = down_tensor.size()[3] - x.size()[3]\n\n        x = torch.nn.functional.pad(x, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([down_tensor,x], dim=1)\n        x = self.conv(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:45:01.725808Z","iopub.execute_input":"2024-07-28T16:45:01.726125Z","iopub.status.idle":"2024-07-28T16:45:01.739883Z","shell.execute_reply.started":"2024-07-28T16:45:01.726061Z","shell.execute_reply":"2024-07-28T16:45:01.739139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### U-Net Model","metadata":{}},{"cell_type":"code","source":"# Defining UNet Model\nclass UNet(torch.nn.Module):\n    # Define the init method\n    def __init__(self, in_channels, out_channels):\n        # Call the super class constructor\n        super(UNet,self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        # Define the Layers\n        # Downsample layers\n        self.down1 = ConvBlock(in_channels, 64)\n        self.down2 = DownsampleBlock(64, 128)\n        self.down3 = DownsampleBlock(128, 256)\n        self.down4 = DownsampleBlock(256, 512)\n        # Bottleneck layers\n        self.b = DownsampleBlock(512, 1024)\n        # Upsample layers\n        self.up1 = UpsampleBlock(1024, 512)\n        self.up2 = UpsampleBlock(512, 256)\n        self.up3 = UpsampleBlock(256, 128)\n        self.up4 = UpsampleBlock(128, 64)\n        self.outc = ConvBlock(64, out_channels)\n\n    # Define the forward method\n    def forward(self, x):\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.b(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x\n    \n    # Define Checkpointing\n    def use_checkpointing(self):\n        self.inc = torch.utils.checkpoint(self.inc)\n        self.down1 = torch.utils.checkpoint(self.down1)\n        self.down2 = torch.utils.checkpoint(self.down2)\n        self.down3 = torch.utils.checkpoint(self.down3)\n        self.down4 = torch.utils.checkpoint(self.down4)\n        self.up1 = torch.utils.checkpoint(self.up1)\n        self.up2 = torch.utils.checkpoint(self.up2)\n        self.up3 = torch.utils.checkpoint(self.up3)\n        self.up4 = torch.utils.checkpoint(self.up4)\n        self.outc = torch.utils.checkpoint(self.outc)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:45:01.740935Z","iopub.execute_input":"2024-07-28T16:45:01.741218Z","iopub.status.idle":"2024-07-28T16:45:01.754045Z","shell.execute_reply.started":"2024-07-28T16:45:01.741195Z","shell.execute_reply":"2024-07-28T16:45:01.753226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display images and masks\ndef compare_segmented(images, preds, masks):\n    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n    for i in range(4):\n        axes[0, i].imshow(images[i].to('cpu').permute(1, 2, 0))\n        axes[0, i].set_title('Image')\n        axes[0, i].axis('off')\n        axes[1, i].imshow(preds[i][0])\n        axes[1, i].set_title('Mask - Predicted')\n        axes[1, i].axis('off')\n        axes[2, i].imshow(masks[i][0].to('cpu'), cmap='gray')\n        axes[2, i].set_title('Mask - Ground Truth')\n        axes[2, i].axis('off')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:45:01.755297Z","iopub.execute_input":"2024-07-28T16:45:01.75566Z","iopub.status.idle":"2024-07-28T16:45:01.764533Z","shell.execute_reply.started":"2024-07-28T16:45:01.755636Z","shell.execute_reply":"2024-07-28T16:45:01.763771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = UNet(in_channels=3,out_channels=1).to(device)  # Assuming binary segmentation\n\n# for images, masks in train_loader:\n#     images = images.to(device)\n#     masks = masks.to(device)\n#     outputs = model(images).detach().cpu()\n#     print(\"Output shape: \",outputs.shape)\n#     compare_segmented(images, outputs, masks)\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:45:01.765435Z","iopub.execute_input":"2024-07-28T16:45:01.7657Z","iopub.status.idle":"2024-07-28T16:45:03.874606Z","shell.execute_reply.started":"2024-07-28T16:45:01.765668Z","shell.execute_reply":"2024-07-28T16:45:03.873712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function and training loop","metadata":{}},{"cell_type":"code","source":"#Creating FocalLoss class for loass funtion\n\n# https://pytorch.org/vision/main/_modules/torchvision/ops/focal_loss.html\nclass FocalLoss(nn.Module):\n    def __init__(self,alpha = 0.25,gamma = 2):\n        super(FocalLoss,self).__init__()\n        self.alpha = 0.25\n        self.gamma = 2\n    \n    \n    def forward(self,inputs,targets):\n        p = torch.sigmoid(inputs)\n        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n        p_t = p * targets + (1 - p) * (1 - targets)\n        loss = ce_loss * ((1 - p_t) ** gamma)\n\n        if alpha >= 0:\n            alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n            loss = alpha_t * loss\n    \n        return loss\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the training loop\ndef train_model(model, train_loader, test_loader,\n                loss_fn, optimizer,epochs=25,\n                device='cpu',save_path='./model.pth'):\n\n    model = model.to(device)\n    best_loss = float('inf')\n    train_score = []\n    test_score = []\n    \n    for epoch in range(epochs):\n        model.train() #training model\n        running_loss = 0.0 #current model loss\n        train_epoch = []\n        \n        for inputs,masks in tqdm(train_loader):\n            inputs = inputs.to(device)\n            masks = masks.to(device)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(inputs)\n            \n            loss = loss_fn(outputs,masks)\n            loss.backword()\n            \n            optimizer.step()\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reference:\n* https://www.kaggle.com/code/christianmariothomas/brain-tumor-segmentaion-unet\n* https://www.kaggle.com/code/akshitsharma1/unet-architecture-explained-in-one-shot-tutorial\n* https://pytorch.org/vision/main/_modules/torchvision/ops/focal_loss.html","metadata":{}}]}